{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "makefile"
    }
   },
   "source": [
    "# Environment Testing (after configuring the server for Detectron2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add Cuda, Conda, and OpenCV to PATH so its available inside Jupyter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Add Conda binary directory to the PATH\n",
    "os.environ['PATH'] += ':/home/sirpoopsalot/miniconda/bin'\n",
    "\n",
    "# Add CUDA binary directory to the PATH\n",
    "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
    "\n",
    "# # Add OpenCV binary directory to the PATH\n",
    "# os.environ['PATH'] += ':/path/to/opencv/bin'  # Replace '/path/to/opencv/bin' with the actual path to your OpenCV binary directory\n",
    "\n",
    "# Add CUDA library directory to LD_LIBRARY_PATH\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64'\n",
    "\n",
    "# test Conda\n",
    "!conda info\n",
    "\n",
    "# test Cuda\n",
    "!nvcc --version\n",
    "\n",
    "# # Test OpenCV\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. make sure we are working from within our new environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Verify Torch is available by listing the version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Verify that Dectron, PyTorch, and are available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. verify that we have GPU processing available to our model\n",
    "\n",
    "   (this will make sure our drivers and libraires are correctly set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets do some testing with Detectron2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. configure the system with a basic setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "#from detectron2.utils.logger import setup_logger\n",
    "#setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "# create work around for google colab cv2_imshow package\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(image):\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lets get started by bringing in an image we want to evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image\n",
    "image_path = \"/mnt/data/skyy-med/model_data/temp_test_data/people.jpg\" \n",
    "my_new_image = cv2.imread(image_path)\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 8))  # Adjust the values (width, height) as needed\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(cv2.cvtColor(my_new_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next lets run the image againsta a pretrained model for testing Keypoint detection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with a keypoint detection model\n",
    "cfg_keypoint = get_cfg()   # get a fresh new config\n",
    "cfg_keypoint.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg_keypoint.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "cfg_keypoint.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg_keypoint)\n",
    "outputs = predictor(my_new_image)\n",
    "v = Visualizer(my_new_image[:,:,::-1], MetadataCatalog.get(cfg_keypoint.DATASETS.TRAIN[0]), scale=3)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 8))  # Adjust the values (width, height) as needed\n",
    "\n",
    "imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Next lets run the image againsta a pretrained model for Testing Instance segmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with instance segmentation\n",
    "cfg_inst = get_cfg()\n",
    "cfg_inst.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg_inst.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# Find a model from detectron2's model zoo.  https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\n",
    "cfg_inst.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg_inst)\n",
    "outputs = predictor(my_new_image)\n",
    "\n",
    "v = Visualizer(my_new_image[:, :, ::-1], MetadataCatalog.get(cfg_inst.DATASETS.TRAIN[0]), scale=1.0)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 8))  # Adjust the values (width, height) as needed\n",
    "\n",
    "imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Next lets run the image against a pretrained model for Testing Panoptic segmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with a panoptic segmentation model\n",
    "cfg_pan = get_cfg()\n",
    "cfg_pan.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
    "cfg_pan.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg_pan)\n",
    "panoptic_seg, segments_info = predictor(my_new_image)[\"panoptic_seg\"]\n",
    "v = Visualizer(my_new_image[:, :, ::-1], MetadataCatalog.get(cfg_pan.DATASETS.TRAIN[0]), scale=1.0)\n",
    "out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 8))  # Adjust the values (width, height) as needed\n",
    "\n",
    "imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Next lets run the pretrained model on something it should not been trained to identify.\n",
    "(Bring in a medical image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image\n",
    "image_path = \"/mnt/data/skyy-med/model_data/x-ray/FracAtlas/images/Fractured/IMG0002311.jpg\" \n",
    "fracture_image = cv2.imread(image_path)\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(12, 10))  # Adjust the values (width, height) as needed\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(cv2.cvtColor(fracture_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Run the model on the medical image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_outputs = predictor(fracture_image)\n",
    "sci_v = Visualizer(fracture_image[:, :, ::-1], MetadataCatalog.get(cfg_inst.DATASETS.TRAIN[0]))\n",
    "sci_out = sci_v.draw_instance_predictions(sci_outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(12, 10))  # Adjust the values (width, height) as needed\n",
    "\n",
    "imshow(sci_out.get_image()[:, :, ::-1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
